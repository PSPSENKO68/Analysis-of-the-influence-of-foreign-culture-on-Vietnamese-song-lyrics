{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T15:16:54.476471Z",
     "start_time": "2025-11-01T15:16:53.893681Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from rapidfuzz import process, fuzz"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T15:29:56.591801Z",
     "start_time": "2025-11-01T15:29:50.962959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "# ===================================================\n",
    "# 1Ô∏è‚É£ H√ÄM ƒê·∫æM S·ªê D√íNG C√ì URL NHAC.VN\n",
    "# ===================================================\n",
    "def count_nhacvn_urls(df):\n",
    "    count_with_nhacvn = 0\n",
    "    total = len(df)\n",
    "    for urls in df['urls']:\n",
    "        if pd.isna(urls):\n",
    "            continue\n",
    "        try:\n",
    "            parsed = ast.literal_eval(urls)\n",
    "            if isinstance(parsed, str):\n",
    "                parsed = [parsed]\n",
    "        except Exception:\n",
    "            parsed = [urls]\n",
    "        if any(isinstance(u, str) and \"nhac.vn/bai-hat/\" in u for u in parsed):\n",
    "            count_with_nhacvn += 1\n",
    "    return count_with_nhacvn, total\n",
    "\n",
    "# ===================================================\n",
    "# 2Ô∏è‚É£ XO√Å C√ÅC URL NHAC.VN + TH√äM C·ªòT MARK\n",
    "# ===================================================\n",
    "def remove_nhacvn_urls_add_mark(input_csv, output_csv):\n",
    "    df = pd.read_csv(input_csv)\n",
    "    before, total = count_nhacvn_urls(df)\n",
    "    print(f\"üéµ Tr∆∞·ªõc khi x√≥a: {before}/{total} d√≤ng ch·ª©a URL nhac.vn ({before/total*100:.2f}%)\")\n",
    "\n",
    "    marks = []\n",
    "\n",
    "    def clean_urls(urls):\n",
    "        if pd.isna(urls) or not isinstance(urls, str):\n",
    "            marks.append(0)\n",
    "            return '[]'\n",
    "        try:\n",
    "            parsed = ast.literal_eval(urls)\n",
    "            if isinstance(parsed, str):\n",
    "                parsed = [parsed]\n",
    "        except Exception:\n",
    "            parsed = [urls]\n",
    "        has_nhacvn = any(isinstance(u, str) and \"nhac.vn/bai-hat/\" in u for u in parsed)\n",
    "        filtered = [u for u in parsed if isinstance(u, str) and \"nhac.vn/bai-hat/\" not in u]\n",
    "        marks.append(1 if has_nhacvn else 0)\n",
    "        return str(filtered) if filtered else '[]'\n",
    "\n",
    "    df['urls'] = df['urls'].apply(clean_urls)\n",
    "\n",
    "    # Th√™m c·ªôt mark sau c·ªôt note (n·∫øu c√≥)\n",
    "    insert_pos = df.columns.get_loc('note') + 1 if 'note' in df.columns else len(df.columns)\n",
    "    df.insert(insert_pos, 'mark', marks)\n",
    "\n",
    "    after, _ = count_nhacvn_urls(df)\n",
    "    print(f\"üßπ Sau khi x√≥a: {after} d√≤ng c√≤n ch·ª©a nhac.vn\")\n",
    "    print(f\"üìç T·ªïng s·ªë d√≤ng mark=1 (ƒë√£ x√≥a nhacvn): {sum(marks)}\")\n",
    "\n",
    "    df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"üíæ ƒê√£ l∆∞u file cleaned + mark t·∫°i: {output_csv}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# ===================================================\n",
    "# 3Ô∏è‚É£ CHU·∫®N H√ìA TEXT & R√öT G·ªåN LYRICS\n",
    "# ===================================================\n",
    "def normalize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    text = unidecode(text.lower())\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def make_signature(title, lyrics, max_words=30):\n",
    "    \"\"\"Gh√©p title v√† 20-30 k√Ω t·ª± ƒë·∫ßu c·ªßa lyrics ƒë·ªÉ so kh·ªõp.\"\"\"\n",
    "    lyrics_part = ' '.join(lyrics.split()[:max_words]) if isinstance(lyrics, str) else ''\n",
    "    return normalize_text(f\"{title} {lyrics_part}\")\n",
    "\n",
    "# ===================================================\n",
    "# 4Ô∏è‚É£ FILL L·∫†I URL D·ª∞A TR√äN MARK==1\n",
    "# ===================================================\n",
    "def refill_urls_marked(final_csv, reference_csv, preview_csv, output_csv, threshold=90):\n",
    "    df_final = pd.read_csv(final_csv)\n",
    "    df_ref = pd.read_csv(reference_csv)\n",
    "\n",
    "    df_ref['signature'] = df_ref.apply(lambda r: make_signature(r['title'], r.get('lyrics', '')), axis=1)\n",
    "    ref_signatures = df_ref['signature'].tolist()\n",
    "    ref_urls = df_ref['urls'].tolist()\n",
    "\n",
    "    preview_rows = []\n",
    "    matched_count = 0\n",
    "\n",
    "    # Ch·ªâ x√©t c√°c h√†ng b·ªã x√≥a nhac.vn\n",
    "    target_rows = df_final[df_final['mark'] == 1]\n",
    "\n",
    "    before_fill, total = count_nhacvn_urls(df_final)\n",
    "    print(f\"üîç Tr∆∞·ªõc khi fill l·∫°i: {before_fill}/{total} d√≤ng c√≥ URL nhac.vn\")\n",
    "\n",
    "    for i, row in target_rows.iterrows():\n",
    "        sig = make_signature(row['title'], row.get('lyrics', ''))\n",
    "        if not sig:\n",
    "            continue\n",
    "\n",
    "        best_match = process.extractOne(sig, ref_signatures, scorer=fuzz.token_set_ratio)\n",
    "        if not best_match:\n",
    "            continue\n",
    "\n",
    "        ref_sig, score, idx = best_match\n",
    "        if score >= threshold:\n",
    "            matched_url = df_ref.iloc[idx]['urls']\n",
    "            preview_rows.append({\n",
    "                'index': i,\n",
    "                'title': row['title'],\n",
    "                'url_new': matched_url,\n",
    "                'similarity': score\n",
    "            })\n",
    "            matched_count += 1\n",
    "            df_final.at[i, 'urls'] = str([matched_url])\n",
    "\n",
    "    df_preview = pd.DataFrame(preview_rows)\n",
    "    df_preview.to_csv(preview_csv, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    after_fill, _ = count_nhacvn_urls(df_final)\n",
    "    print(f\"‚úÖ ƒê√£ g√°n th√™m {matched_count} URL nhac.vn m·ªõi (threshold ‚â• {threshold})\")\n",
    "    print(f\"üìà Sau khi fill l·∫°i: {after_fill}/{total} d√≤ng c√≥ URL nhac.vn\")\n",
    "\n",
    "    df_final.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"üíæ ƒê√£ l∆∞u final_dataset m·ªõi t·∫°i: {output_csv}\")\n",
    "    print(f\"üëÅÔ∏è‚Äçüó®Ô∏è Preview l∆∞u t·∫°i: {preview_csv}\")\n",
    "\n",
    "    return df_preview, df_final\n",
    "\n",
    "# ===================================================\n",
    "# üöÄ CH·∫†Y QUY TR√åNH\n",
    "# ===================================================\n",
    "# B∆∞·ªõc 1: X√≥a + ƒë√°nh d·∫•u mark\n",
    "cleaned_df = remove_nhacvn_urls_add_mark(\"final_dataset_cleaned_v2.csv\", \"final_dataset_no_nhacvn.csv\")\n",
    "\n",
    "# B∆∞·ªõc 2: Fill l·∫°i d·ª±a tr√™n title + 20-30 t·ª´ ƒë·∫ßu c·ªßa lyrics\n",
    "#preview, final = refill_urls_marked(\n",
    " #   final_csv=\"final_dataset_no_nhacvn.csv\",\n",
    "  #  reference_csv=\"outputNhacvn2.csv\",\n",
    "   # preview_csv=\"preview_refill_marked.csv\",\n",
    "    ##hreshold=85\n",
    "#)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéµ Tr∆∞·ªõc khi x√≥a: 20671/62289 d√≤ng ch·ª©a URL nhac.vn (33.19%)\n",
      "üßπ Sau khi x√≥a: 0 d√≤ng c√≤n ch·ª©a nhac.vn\n",
      "üìç T·ªïng s·ªë d√≤ng mark=1 (ƒë√£ x√≥a nhacvn): 20671\n",
      "üíæ ƒê√£ l∆∞u file cleaned + mark t·∫°i: final_dataset_no_nhacvn.csv\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T15:30:29.659086Z",
     "start_time": "2025-11-01T15:30:29.631348Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from rapidfuzz import process, fuzz\n",
    "import ast\n",
    "from math import ceil\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "def normalize_text_simple(s):\n",
    "    \"\"\"Lower, remove diacritics, remove non-alnum (keeps spaces).\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return ''\n",
    "    s = unidecode(s).lower()\n",
    "    s = re.sub(r'[^a-z0-9\\s]', ' ', s)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s\n",
    "\n",
    "def signature_from_parts(title, creator, lyrics_snip, max_chars=30):\n",
    "    \"\"\"Build a normalized signature from title + creator + short lyrics snippet.\"\"\"\n",
    "    title_n = normalize_text_simple(title)\n",
    "    creator_n = normalize_text_simple(creator)\n",
    "    lyrics_n = normalize_text_simple((lyrics_snip or '')[:max_chars])\n",
    "    # order: title, creator, lyrics snippet\n",
    "    return (title_n + ' ' + creator_n + ' ' + lyrics_n).strip()\n",
    "\n",
    "def safe_eval_list(x):\n",
    "    \"\"\"Try to interpret x as Python list; fallback to [x] or [].\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    try:\n",
    "        parsed = ast.literal_eval(x)\n",
    "        if isinstance(parsed, (list, tuple)):\n",
    "            return list(parsed)\n",
    "        if isinstance(parsed, str):\n",
    "            return [parsed]\n",
    "        return [str(parsed)]\n",
    "    except Exception:\n",
    "        # fallback: try split by comma if it looks like multiple, else single\n",
    "        if isinstance(x, str) and ',' in x:\n",
    "            parts = [p.strip() for p in x.split(',') if p.strip()]\n",
    "            return parts\n",
    "        return [x] if x else []\n",
    "\n",
    "# -------------------------\n",
    "# Main function\n",
    "# -------------------------\n",
    "def refill_using_title_composer_lyrics(\n",
    "    final_csv,\n",
    "    reference_csv,\n",
    "    preview_csv=\"preview_replace_title_comp_lyrics.csv\",\n",
    "    output_filled_csv=\"final_dataset_filled.csv\",\n",
    "    batch_size=4000,\n",
    "    similarity_threshold=85,\n",
    "    lyrics_chars=30,\n",
    "    do_replace=False,\n",
    "    save_partial_every=1\n",
    "):\n",
    "    \"\"\"\n",
    "    - final_csv: final_dataset_no_nhacvn.csv (must have 'mark' column)\n",
    "    - reference_csv: outputNhacvn2.csv (must have 'urls','title','composer','lyrics' columns)\n",
    "    - preview_csv: output preview mapping index -> new url (saved progressively per batch)\n",
    "    - do_replace: if True, apply replacements into a copy and save output_filled_csv\n",
    "    - similarity_threshold: minimal score to accept a match (0-100)\n",
    "    - lyrics_chars: number of characters taken from lyrics for signature\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Loading data...\")\n",
    "    df_final = pd.read_csv(final_csv)\n",
    "    df_ref = pd.read_csv(reference_csv)\n",
    "\n",
    "    # Ensure required columns exist\n",
    "    for c in ['urls', 'title']:\n",
    "        if c not in df_ref.columns:\n",
    "            raise ValueError(f\"Reference file missing required column: {c}\")\n",
    "\n",
    "    # Normalize and prepare reference dataframe (reset index)\n",
    "    df_ref = df_ref[['urls', 'title'] + ([c for c in ['composer','lyrics'] if c in df_ref.columns])].copy()\n",
    "    df_ref = df_ref.reset_index(drop=True)\n",
    "    df_ref['title_n'] = df_ref['title'].astype(str).apply(normalize_text_simple)\n",
    "    df_ref['composer_n'] = df_ref['composer'].astype(str).apply(normalize_text_simple) if 'composer' in df_ref.columns else ''\n",
    "    df_ref['lyrics_n'] = df_ref['lyrics'].astype(str).apply(normalize_text_simple) if 'lyrics' in df_ref.columns else ''\n",
    "    # Precompute small lyrics snippets used in signature\n",
    "    df_ref['lyrics_snip'] = df_ref['lyrics_n'].str[:lyrics_chars]\n",
    "\n",
    "    # Build reference signatures (full) used as fallback pool\n",
    "    df_ref['full_signature'] = df_ref.apply(\n",
    "        lambda r: signature_from_parts(r['title'], r.get('composer', ''), r.get('lyrics_snip', ''), max_chars=lyrics_chars),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Indexing maps for quick prefilter:\n",
    "    # by first word of title -> list of indices\n",
    "    df_ref['first_word'] = df_ref['title_n'].str.split().str[0].fillna('')\n",
    "    by_first_word = df_ref.groupby('first_word').apply(lambda g: g.index.tolist()).to_dict()\n",
    "    # by composer exact token -> indices\n",
    "    df_ref['composer_token'] = df_ref['composer_n'].str.split().str[0].fillna('')\n",
    "    by_composer = df_ref.groupby('composer_token').apply(lambda g: g.index.tolist()).to_dict()\n",
    "\n",
    "    # Filter final rows to only those marked (mark==1)\n",
    "    if 'mark' not in df_final.columns:\n",
    "        raise ValueError(\"final dataset must have 'mark' column (1 = needs fill).\")\n",
    "    df_target = df_final[df_final['mark'] == 1].copy()\n",
    "    print(f\"üéØ Rows to process (mark==1): {len(df_target)}\")\n",
    "\n",
    "    # We'll accumulate preview mappings here\n",
    "    preview_records = []\n",
    "\n",
    "    # For replacement, operate on a copy if requested\n",
    "    if do_replace:\n",
    "        df_out = df_final.copy()\n",
    "    else:\n",
    "        df_out = None\n",
    "\n",
    "    total = len(df_target)\n",
    "    batches = ceil(total / batch_size) if batch_size > 0 else 1\n",
    "\n",
    "    processed = 0\n",
    "    for b in range(batches):\n",
    "        start = b * batch_size\n",
    "        end = min(start + batch_size, total)\n",
    "        batch = df_target.iloc[start:end]\n",
    "        print(f\"\\n‚öôÔ∏è Processing batch {b+1}/{batches} - rows {start}:{end} ...\")\n",
    "\n",
    "        # For each row in batch, create query signature and build candidate pool\n",
    "        for idx, row in batch.iterrows():\n",
    "            title = str(row.get('title','')).strip()\n",
    "            if not title:\n",
    "                continue\n",
    "            # choose creator: composer first, else lyricist if present in final dataset\n",
    "            creator = ''\n",
    "            if 'composers' in row and pd.notna(row.get('composers','')) and str(row.get('composers','')).strip():\n",
    "                # composers in final dataset might be string/list; make simple string\n",
    "                comp_val = row.get('composers')\n",
    "                try:\n",
    "                    comp_parsed = ast.literal_eval(comp_val) if isinstance(comp_val, str) and comp_val.strip().startswith('[') else comp_val\n",
    "                except Exception:\n",
    "                    comp_parsed = comp_val\n",
    "                if isinstance(comp_parsed, list):\n",
    "                    creator = ' '.join(map(str, comp_parsed))\n",
    "                else:\n",
    "                    creator = str(comp_parsed)\n",
    "            elif 'lyricists' in row and pd.notna(row.get('lyricists','')) and str(row.get('lyricists','')).strip():\n",
    "                lyr_val = row.get('lyricists')\n",
    "                try:\n",
    "                    lyr_parsed = ast.literal_eval(lyr_val) if isinstance(lyr_val, str) and lyr_val.strip().startswith('[') else lyr_val\n",
    "                except Exception:\n",
    "                    lyr_parsed = lyr_val\n",
    "                if isinstance(lyr_parsed, list):\n",
    "                    creator = ' '.join(map(str, lyr_parsed))\n",
    "                else:\n",
    "                    creator = str(lyr_parsed)\n",
    "            creator_n = normalize_text_simple(creator)\n",
    "\n",
    "            # lyrics snippet: prefer final_dataset lyrics if exists (short), else empty\n",
    "            lyrics_val = row.get('lyrics','') if 'lyrics' in row else ''\n",
    "            lyrics_snip = normalize_text_simple(str(lyrics_val))[:lyrics_chars]\n",
    "\n",
    "            # Build query signature\n",
    "            query_sig = signature_from_parts(title, creator, lyrics_snip, max_chars=lyrics_chars)\n",
    "\n",
    "            # Build candidate indices by priority:\n",
    "            candidate_indices = []\n",
    "\n",
    "            # 1) If creator token exists and matches some ref composers -> use those first\n",
    "            comp_token = creator_n.split()[0] if creator_n else ''\n",
    "            if comp_token and comp_token in by_composer:\n",
    "                candidate_indices.extend(by_composer[comp_token])\n",
    "\n",
    "            # 2) Prefilter by first_word of title\n",
    "            first_word = normalize_text_simple(title).split()[0] if title.split() else ''\n",
    "            if first_word and first_word in by_first_word:\n",
    "                # extend but avoid duplicates\n",
    "                for ci in by_first_word[first_word]:\n",
    "                    if ci not in candidate_indices:\n",
    "                        candidate_indices.append(ci)\n",
    "\n",
    "            # 3) fallback: if still empty or to increase recall, add entire reference\n",
    "            if not candidate_indices:\n",
    "                candidate_indices = df_ref.index.tolist()\n",
    "\n",
    "            # Build candidate signatures list (ordered) and a map to global indices\n",
    "            candidate_sigs = df_ref.loc[candidate_indices, 'full_signature'].tolist()\n",
    "            candidate_idx_map = list(candidate_indices)  # candidate_idx_map[pos] -> global idx\n",
    "\n",
    "            # If candidate_sigs empty, skip\n",
    "            if not candidate_sigs:\n",
    "                preview_records.append({\n",
    "                    'index': idx,\n",
    "                    'title': title,\n",
    "                    'urls_old': row.get('urls',''),\n",
    "                    'url_refer': '',\n",
    "                    'similarity': 0\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            # Do the fuzzy search on candidate_sigs\n",
    "            best = process.extractOne(query_sig, candidate_sigs, scorer=fuzz.token_set_ratio)\n",
    "            matched_url = ''\n",
    "            score = 0\n",
    "            if best:\n",
    "                # best may be (match, score) or (match, score, pos)\n",
    "                # Normalize unpacking:\n",
    "                if len(best) == 2:\n",
    "                    matched_sig, score = best[0], best[1]\n",
    "                    pos = candidate_sigs.index(matched_sig) if matched_sig in candidate_sigs else None\n",
    "                else:\n",
    "                    matched_sig, score, pos = best[0], best[1], best[2]\n",
    "                # if pos provided, map to global index\n",
    "                if pos is not None and 0 <= int(pos) < len(candidate_idx_map):\n",
    "                    global_idx = candidate_idx_map[int(pos)]\n",
    "                    matched_url = df_ref.at[global_idx, 'urls']\n",
    "                else:\n",
    "                    # fallback: try to find matched_sig position\n",
    "                    try:\n",
    "                        pos2 = candidate_sigs.index(matched_sig)\n",
    "                        global_idx = candidate_idx_map[pos2]\n",
    "                        matched_url = df_ref.at[global_idx, 'urls']\n",
    "                    except Exception:\n",
    "                        matched_url = ''\n",
    "            # Append preview record\n",
    "            preview_records.append({\n",
    "                'index': idx,\n",
    "                'title': title,\n",
    "                'urls_old': row.get('urls',''),\n",
    "                'url_refer': matched_url,\n",
    "                'similarity': int(score)\n",
    "            })\n",
    "\n",
    "            # If do_replace and score >= threshold -> perform replacement in df_out\n",
    "            if do_replace and matched_url and score >= similarity_threshold:\n",
    "                # replace in df_out: replace any nhac.vn url by the matched_url, keep others\n",
    "                old_urls = safe_eval_list(df_out.at[idx, 'urls'])\n",
    "                # remove all nhac.vn links\n",
    "                filtered = [u for u in old_urls if not (isinstance(u, str) and \"nhac.vn/bai-hat/\" in u)]\n",
    "                filtered.insert(0, matched_url)  # insert new ref at front\n",
    "                df_out.at[idx, 'urls'] = str(filtered)\n",
    "\n",
    "            processed += 1\n",
    "            if processed % 500 == 0:\n",
    "                print(f\"  ‚Ä¢ processed {processed}/{total} rows...\")\n",
    "\n",
    "        # After finishing batch, save partial preview\n",
    "        # convert to dataframe and save (overwrite) so partial results persist\n",
    "        df_preview_partial = pd.DataFrame(preview_records)\n",
    "        df_preview_partial.to_csv(preview_csv, index=False, encoding='utf-8-sig')\n",
    "        print(f\"üíæ Saved partial preview ({len(preview_records)} rows) to {preview_csv}\")\n",
    "\n",
    "    # Final summary\n",
    "    df_preview = pd.DataFrame(preview_records)\n",
    "    matched_count = df_preview['url_refer'].astype(bool).sum()\n",
    "    high_conf = df_preview[df_preview['similarity'] >= similarity_threshold].shape[0]\n",
    "\n",
    "    print(\"\\n‚úÖ Done.\")\n",
    "    print(f\"üîé Total preview rows: {len(df_preview)}\")\n",
    "    print(f\"üîó Found candidate url_refer (any score): {matched_count}\")\n",
    "    print(f\"‚úÖ High-confidence matches (>= {similarity_threshold}): {high_conf}\")\n",
    "\n",
    "    # Save final preview\n",
    "    df_preview.to_csv(preview_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"üìÅ Final preview written to: {preview_csv}\")\n",
    "\n",
    "    # If do_replace, save output file\n",
    "    if do_replace:\n",
    "        df_out.to_csv(output_filled_csv, index=False, encoding='utf-8-sig')\n",
    "        print(f\"üíæ Replacements applied and saved to: {output_filled_csv}\")\n",
    "\n",
    "    return df_preview"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T15:38:29.292883Z",
     "start_time": "2025-11-01T15:30:32.191004Z"
    }
   },
   "source": [
    "preview = refill_using_title_composer_lyrics(\n",
    "    final_csv=\"final_dataset_no_nhacvn.csv\",\n",
    "    reference_csv=\"outputNhacvn2.csv\",\n",
    "    preview_csv=\"preview_replace_title_comp_lyrics.csv\",\n",
    "    output_filled_csv=\"final_dataset_filled.csv\",\n",
    "    batch_size=4000,\n",
    "    similarity_threshold=90,\n",
    "    lyrics_chars=50,\n",
    "    do_replace=False\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dm/b6833525473fb0p2gk_x1dmm0000gp/T/ipykernel_41590/3147350715.py:97: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  by_first_word = df_ref.groupby('first_word').apply(lambda g: g.index.tolist()).to_dict()\n",
      "/var/folders/dm/b6833525473fb0p2gk_x1dmm0000gp/T/ipykernel_41590/3147350715.py:100: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  by_composer = df_ref.groupby('composer_token').apply(lambda g: g.index.tolist()).to_dict()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Rows to process (mark==1): 20671\n",
      "\n",
      "‚öôÔ∏è Processing batch 1/6 - rows 0:4000 ...\n",
      "  ‚Ä¢ processed 500/20671 rows...\n",
      "  ‚Ä¢ processed 1000/20671 rows...\n",
      "  ‚Ä¢ processed 1500/20671 rows...\n",
      "  ‚Ä¢ processed 2000/20671 rows...\n",
      "  ‚Ä¢ processed 2500/20671 rows...\n",
      "  ‚Ä¢ processed 3000/20671 rows...\n",
      "  ‚Ä¢ processed 3500/20671 rows...\n",
      "  ‚Ä¢ processed 4000/20671 rows...\n",
      "üíæ Saved partial preview (4000 rows) to preview_replace_title_comp_lyrics.csv\n",
      "\n",
      "‚öôÔ∏è Processing batch 2/6 - rows 4000:8000 ...\n",
      "  ‚Ä¢ processed 4500/20671 rows...\n",
      "  ‚Ä¢ processed 5000/20671 rows...\n",
      "  ‚Ä¢ processed 5500/20671 rows...\n",
      "  ‚Ä¢ processed 6000/20671 rows...\n",
      "  ‚Ä¢ processed 6500/20671 rows...\n",
      "  ‚Ä¢ processed 7000/20671 rows...\n",
      "  ‚Ä¢ processed 7500/20671 rows...\n",
      "  ‚Ä¢ processed 8000/20671 rows...\n",
      "üíæ Saved partial preview (8000 rows) to preview_replace_title_comp_lyrics.csv\n",
      "\n",
      "‚öôÔ∏è Processing batch 3/6 - rows 8000:12000 ...\n",
      "  ‚Ä¢ processed 8500/20671 rows...\n",
      "  ‚Ä¢ processed 9000/20671 rows...\n",
      "  ‚Ä¢ processed 9500/20671 rows...\n",
      "  ‚Ä¢ processed 10000/20671 rows...\n",
      "  ‚Ä¢ processed 10500/20671 rows...\n",
      "  ‚Ä¢ processed 11000/20671 rows...\n",
      "  ‚Ä¢ processed 11500/20671 rows...\n",
      "  ‚Ä¢ processed 12000/20671 rows...\n",
      "üíæ Saved partial preview (12000 rows) to preview_replace_title_comp_lyrics.csv\n",
      "\n",
      "‚öôÔ∏è Processing batch 4/6 - rows 12000:16000 ...\n",
      "  ‚Ä¢ processed 12500/20671 rows...\n",
      "  ‚Ä¢ processed 13000/20671 rows...\n",
      "  ‚Ä¢ processed 13500/20671 rows...\n",
      "  ‚Ä¢ processed 14000/20671 rows...\n",
      "  ‚Ä¢ processed 14500/20671 rows...\n",
      "  ‚Ä¢ processed 15000/20671 rows...\n",
      "  ‚Ä¢ processed 15500/20671 rows...\n",
      "  ‚Ä¢ processed 16000/20671 rows...\n",
      "üíæ Saved partial preview (16000 rows) to preview_replace_title_comp_lyrics.csv\n",
      "\n",
      "‚öôÔ∏è Processing batch 5/6 - rows 16000:20000 ...\n",
      "  ‚Ä¢ processed 16500/20671 rows...\n",
      "  ‚Ä¢ processed 17000/20671 rows...\n",
      "  ‚Ä¢ processed 17500/20671 rows...\n",
      "  ‚Ä¢ processed 18000/20671 rows...\n",
      "  ‚Ä¢ processed 18500/20671 rows...\n",
      "  ‚Ä¢ processed 19000/20671 rows...\n",
      "  ‚Ä¢ processed 19500/20671 rows...\n",
      "  ‚Ä¢ processed 20000/20671 rows...\n",
      "üíæ Saved partial preview (20000 rows) to preview_replace_title_comp_lyrics.csv\n",
      "\n",
      "‚öôÔ∏è Processing batch 6/6 - rows 20000:20671 ...\n",
      "  ‚Ä¢ processed 20500/20671 rows...\n",
      "üíæ Saved partial preview (20671 rows) to preview_replace_title_comp_lyrics.csv\n",
      "\n",
      "‚úÖ Done.\n",
      "üîé Total preview rows: 20671\n",
      "üîó Found candidate url_refer (any score): 20671\n",
      "‚úÖ High-confidence matches (>= 90): 18771\n",
      "üìÅ Final preview written to: preview_replace_title_comp_lyrics.csv\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T15:40:57.144190Z",
     "start_time": "2025-11-01T15:40:50.120958Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "def simple_fill_urls_by_index(final_file, preview_file, output_file):\n",
    "    # ƒê·ªçc file\n",
    "    df_final = pd.read_csv(final_file)\n",
    "    df_preview = pd.read_csv(preview_file)\n",
    "\n",
    "    # ƒê·∫£m b·∫£o c·ªôt index l√† int\n",
    "    df_preview['index'] = df_preview['index'].astype(int)\n",
    "\n",
    "    # D√πng set cho tra c·ª©u nhanh\n",
    "    preview_map = dict(zip(df_preview['index'], df_preview['url_refer']))\n",
    "\n",
    "    filled = 0\n",
    "    for i, row in df_final.iterrows():\n",
    "        if i in preview_map:\n",
    "            new_url = preview_map[i]\n",
    "            if isinstance(new_url, str) and new_url.strip():\n",
    "                try:\n",
    "                    urls = ast.literal_eval(row['urls']) if isinstance(row['urls'], str) else []\n",
    "                    if not isinstance(urls, list):\n",
    "                        urls = [urls]\n",
    "                except Exception:\n",
    "                    urls = []\n",
    "\n",
    "                urls.append(new_url)\n",
    "                # lo·∫°i tr√πng + stringify\n",
    "                df_final.at[i, 'urls'] = str(list(set(urls)))\n",
    "                filled += 1\n",
    "\n",
    "    df_final.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "    print(f\"‚úÖ ƒê√£ fill l·∫°i {filled} d√≤ng d·ª±a theo index\")\n",
    "    print(f\"üíæ ƒê√£ l∆∞u file k·∫øt qu·∫£: {output_file}\")\n",
    "    return df_final\n",
    "\n",
    "\n",
    "# üß© Ch·∫°y l·ªánh\n",
    "filled_df = simple_fill_urls_by_index(\n",
    "    final_file=\"vietnamese-stopwords/final_dataset_no_nhacvn.csv\",\n",
    "    preview_file=\"preview_replace_title_comp_lyrics.csv\",\n",
    "    output_file=\"vietnamese-stopwords/final_dataset_cleaned_v3.csv\"\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ fill l·∫°i 20671 d√≤ng d·ª±a theo index\n",
      "üíæ ƒê√£ l∆∞u file k·∫øt qu·∫£: final_dataset_cleaned_v3.csv\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T03:23:06.701838Z",
     "start_time": "2025-11-02T03:23:02.772502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import ast # D√πng ƒë·ªÉ chuy·ªÉn string \"['a', 'b']\" th√†nh list ['a', 'b']\n",
    "from urllib.parse import urlparse # D√πng ƒë·ªÉ ph√¢n t√≠ch URL\n",
    "import sys # D√πng ƒë·ªÉ tho√°t ch∆∞∆°ng tr√¨nh n·∫øu c√≥ l·ªói\n",
    "\n",
    "# --- ‚¨áÔ∏è B∆Ø·ªöC 1: C·∫§U H√åNH FILE V√Ä C·ªòT ‚¨áÔ∏è ---\n",
    "\n",
    "# 1. ƒêi·ªÅn ƒë∆∞·ªùng d·∫´n (path) ƒë·∫øn file c·ªßa b·∫°n\n",
    "# V√≠ d·ª•: 'data/dataset_cua_toi.csv' ho·∫∑c 'D:/Project/data.xlsx'\n",
    "file_path = 'vietnamese-stopwords/final_dataset_cleaned_v3.csv'  # <-- THAY ƒê·ªîI PATH N√ÄY\n",
    "\n",
    "# 2. ƒêi·ªÅn t√™n ch√≠nh x√°c c·ªßa c·ªôt ch·ª©a URLs\n",
    "url_column_name = 'urls' # <-- THAY ƒê·ªîI T√äN C·ªòT N√ÄY (n·∫øu c·∫ßn)\n",
    "\n",
    "# -----------------------------------------------\n",
    "\n",
    "# --- B∆Ø·ªöC 2: ƒê·ªåC FILE D·ªÆ LI·ªÜU ---\n",
    "try:\n",
    "    if file_path.endswith('.csv'):\n",
    "        df = pd.read_csv(file_path)\n",
    "    elif file_path.endswith('.xlsx') or file_path.endswith('.xls'):\n",
    "        df = pd.read_excel(file_path)\n",
    "    else:\n",
    "        print(f\"L·ªñI: Kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c ƒëu√¥i file. Ch·ªâ h·ªó tr·ª£ .csv, .xlsx, .xls\")\n",
    "        print(\"ƒêang th·ª≠ ƒë·ªçc b·∫±ng .csv...\")\n",
    "        df = pd.read_csv(file_path) # Th·ª≠ m·∫∑c ƒë·ªãnh l√† CSV\n",
    "\n",
    "    print(f\"‚úÖ ƒê√£ t·∫£i file th√†nh c√¥ng: {file_path}\")\n",
    "\n",
    "    # Ki·ªÉm tra xem c·ªôt URL c√≥ t·ªìn t·∫°i kh√¥ng\n",
    "    if url_column_name not in df.columns:\n",
    "        print(f\"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y c·ªôt t√™n l√† '{url_column_name}' trong file.\")\n",
    "        print(f\"C√°c c·ªôt c√≥ s·∫µn l√†: {list(df.columns)}\")\n",
    "        sys.exit() # D·ª´ng ch∆∞∆°ng tr√¨nh\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y file t·∫°i ƒë∆∞·ªùng d·∫´n: {file_path}\")\n",
    "    sys.exit() # D·ª´ng ch∆∞∆°ng tr√¨nh\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªñI: C√≥ v·∫•n ƒë·ªÅ khi ƒë·ªçc file. L·ªói: {e}\")\n",
    "    sys.exit() # D·ª´ng ch∆∞∆°ng tr√¨nh\n",
    "\n",
    "\n",
    "# --- B∆Ø·ªöC 3: X·ª¨ L√ù V√Ä TR√çCH XU·∫§T DOMAIN (Gi·ªëng nh∆∞ tr∆∞·ªõc) ---\n",
    "\n",
    "# H√†m chuy·ªÉn string \"['a']\" th√†nh list ['a'] m·ªôt c√°ch an to√†n\n",
    "def safe_parse_list(data_str):\n",
    "    if pd.isna(data_str):\n",
    "        return [] # Tr·∫£ v·ªÅ list r·ªóng n·∫øu l√† NaN\n",
    "    try:\n",
    "        # ast.literal_eval s·∫Ω ƒë√°nh gi√° string th√†nh ƒë·ªëi t∆∞·ª£ng Python\n",
    "        return ast.literal_eval(data_str)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return [] # Tr·∫£ v·ªÅ list r·ªóng n·∫øu string kh√¥ng ƒë√∫ng ƒë·ªãnh d·∫°ng\n",
    "\n",
    "# √Åp d·ª•ng h√†m n√†y cho c·ªôt 'urls' c·ªßa b·∫°n\n",
    "df['url_list'] = df[url_column_name].apply(safe_parse_list)\n",
    "\n",
    "# T√°ch m·ªói URL trong list ra th√†nh m·ªôt d√≤ng ri√™ng\n",
    "df_exploded = df.explode('url_list')\n",
    "\n",
    "# H√†m tr√≠ch xu·∫•t domain t·ª´ m·ªói URL\n",
    "def get_domain(url):\n",
    "    if pd.isna(url):\n",
    "        return None\n",
    "    try:\n",
    "        # urlparse(url).netloc s·∫Ω tr·∫£ v·ªÅ ph·∫ßn domain\n",
    "        # V√≠ d·ª•: 'https://nhac.vn/bai-hat/...' -> 'nhac.vn'\n",
    "        return urlparse(url).netloc\n",
    "    except:\n",
    "        return None # N·∫øu URL b·ªã l·ªói\n",
    "\n",
    "# √Åp d·ª•ng h√†m get_domain cho c·ªôt 'url_list'\n",
    "df_exploded['domain'] = df_exploded['url_list'].apply(get_domain)\n",
    "\n",
    "# L·∫•y danh s√°ch c√°c domain duy nh·∫•t\n",
    "unique_domains = df_exploded['domain'].dropna().unique()\n",
    "\n",
    "print(\"\\n--- K·∫æT QU·∫¢ ---\")\n",
    "print(\"Danh s√°ch c√°c trang web (domain) duy nh·∫•t ƒë√£ c√†o l√†:\")\n",
    "# In ra d·∫°ng list cho d·ªÖ ƒë·ªçc v√† copy\n",
    "print(list(unique_domains))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ t·∫£i file th√†nh c√¥ng: final_dataset_cleaned_v3.csv\n",
      "\n",
      "--- K·∫æT QU·∫¢ ---\n",
      "Danh s√°ch c√°c trang web (domain) duy nh·∫•t ƒë√£ c√†o l√†:\n",
      "['lyric.tkaraoke.com', 'loibaihat.co', 'nhac.vn', 'timbaihat.com', 'hopamviet.vn', 'nhachayvn.net', 'keeng.vn']\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
