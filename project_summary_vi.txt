1. Tóm tắt (Abstract)

Đề tài xây dựng một pipeline hoàn chỉnh từ thu thập dữ liệu, chuẩn hóa, gán nhãn đến phân tích thống kê nhằm đo lường mức độ ảnh hưởng của văn hóa nước ngoài lên lời bài hát tiếng Việt. Hệ thống tự động thu thập lời bài hát từ nhiều nguồn trực tuyến (Nhac.vn, TKaraoke, …), chuẩn hóa về một schema thống nhất, sau đó áp dụng một chuỗi bước xử lý văn bản kết hợp từ điển chuyên biệt và mô hình ngôn ngữ lớn (LLM) để phân loại token theo nguồn gốc ngôn ngữ (Việt, Hán Việt, tiếng Anh, phiên âm Hàn/Nhật/Trung/..., tên riêng, từ ngoại lai, …). Trên tập dữ liệu đã gán nhãn, dự án thực hiện các phân tích theo thời gian, theo thể loại và theo tác giả, đồng thời trực quan hóa các xu hướng sử dụng tiếng Anh, từ phiên âm K‑pop, yếu tố Hán Việt, … qua các giai đoạn từ 1990 đến 2025. Kết quả cho thấy tỷ lệ yếu tố nước ngoài, đặc biệt là tiếng Anh và phiên âm tiếng Hàn, gia tăng rõ rệt sau năm 2015, với sự khác biệt mạnh giữa các thể loại (như Rap/Hip-hop so với Ballad).

2. Mục tiêu (Objective)

Mục tiêu tổng quát của dự án là định lượng và mô tả một cách khách quan mức độ du nhập và thay đổi của yếu tố ngoại văn hóa trong lời bài hát tiếng Việt. Cụ thể, hệ thống hướng tới các câu hỏi nghiên cứu: (i) Tỷ lệ xuất hiện của các yếu tố ngôn ngữ nước ngoài (tiếng Anh, từ phiên âm Hàn/Nhật/Trung, Hán Việt, …) trong lời bài hát tiếng Việt thay đổi như thế nào theo thời gian; (ii) Sự phân bố của các yếu tố này giữa các thể loại nhạc và giữa các nhạc sĩ khác nhau có gì khác biệt; (iii) Tầm quan trọng tương đối của từng nhóm yếu tố (ví dụ Hán Việt so với tiếng Anh, phiên âm K‑pop so với ngoại ngữ khác) trong tổng số thành phần phi thuần Việt của ca từ. Dự án không dừng ở nhận định chủ quan mà xây dựng một pipeline lặp lại được, dựa trên luật và từ điển, có thể áp dụng cho các tập lời bài hát lớn.

3. Công nghệ sử dụng (Technologies)

Dự án được cài đặt chủ yếu bằng Python. Các thư viện chính bao gồm: pandas và numpy cho xử lý dữ liệu dạng bảng và tính toán; requests, BeautifulSoup và sqlite3 cho thu thập dữ liệu web và lưu trữ tạm thời; re, unicodedata, unidecode và ahocorasick cho chuẩn hóa và dò tìm mẫu trong văn bản; nltk (corpus words) và rapidfuzz cho từ điển tiếng Anh và đo độ tương đồng chuỗi; matplotlib và seaborn cho trực quan hóa kết quả; langdetect cho nhận diện ngôn ngữ; groq, transformers, torch và BitsAndBytesConfig cho việc gọi và triển khai mô hình ngôn ngữ lớn (LLM) phục vụ gán nhãn. Quá trình thực thi tận dụng tqdm, logging, threading và concurrent.futures để theo dõi tiến độ, ghi log và tăng tốc bằng xử lý song song.

4. Kiến trúc / Tổng quan hệ thống (Architecture / System Overview)

Hệ thống được thiết kế thành ba tầng chính: (1) Tầng thu thập dữ liệu (Data_Crawler), (2) Tầng chuẩn hóa và hợp nhất dữ liệu (Data_Standardized), và (3) Tầng gán nhãn và phân tích (Calculate_Analysis). Tầng đầu tiên tập trung vào việc khai thác lời bài hát và metadata từ các website âm nhạc Việt Nam thông qua các bộ thu thập chuyên biệt cho từng nguồn. Tầng thứ hai chuyển đổi các file CSV với cấu trúc khác nhau về một schema thống nhất, đồng thời hợp nhất nhiều nguồn bằng phát hiện trùng lặp dựa trên tiêu đề, tác giả và mức độ tương đồng lời bài hát. Tầng thứ ba xây dựng từ điển, tiền xử lý câu chữ, gán nhãn từng token theo nguồn gốc ngôn ngữ, sau đó tính toán thống kê và vẽ đồ thị để phân tích xu hướng theo thời gian, thể loại và tác giả. Mặc dù có sử dụng LLM, phần lõi của pipeline là các luật và từ điển có thể tái lập, hỗ trợ đánh giá có hệ thống ảnh hưởng văn hóa nước ngoài lên ca từ.

5. Luồng dữ liệu / Pipeline xử lý (Data Flow / Processing Pipeline)

Luồng dữ liệu bắt đầu từ các trình thu thập (crawler) truy cập sitemap hoặc chức năng tìm kiếm của các website như Nhac.vn và TKaraoke để trích xuất đường dẫn bài hát, sau đó tải về tiêu đề, nghệ sĩ, nhạc sĩ, thể loại và lời bài hát. Dữ liệu thô được lưu dưới dạng CSV và/hoặc trong cơ sở dữ liệu SQLite, đi kèm các file checkpoint để tiếp tục khi dừng giữa chừng. Ở bước chuẩn hóa, từng nguồn được ánh xạ cột về schema chung gồm các trường như tiêu đề, tác giả, năm, thể loại, lời bài hát, URL, nguồn và ghi chú; các danh sách (ví dụ nhiều thể loại) được chuẩn hóa về định dạng thống nhất. Các bộ dữ liệu nguồn sau đó được hợp nhất bằng thuật toán phát hiện trùng lặp sử dụng kết hợp so khớp tiêu đề–tác giả và độ tương đồng lời bài hát (rapidfuzz) với các ngưỡng và bộ lọc độ dài để giảm so sánh không cần thiết. Tiếp đó, năm phát hành bị thiếu được điền bổ sung thông qua chuỗi lời gọi API đến MusicBrainz, Wikipedia và iTunes, với trích xuất năm bằng biểu thức chính quy. Sau giai đoạn chuẩn hóa, lyrics được truyền qua pipeline tiền xử lý nhiều bước: chuẩn hóa Unicode, loại bỏ ký tự gây nhiễu, thay thế teencode, loại bỏ “noise” ở mức từ và cụm từ, rồi lần lượt gán nhãn phiên âm ngoại ngữ, tên riêng, từ Hán Việt, từ Việt và từ Anh bằng các từ điển và tự động Aho‑Corasick. Cuối cùng, các đặc trưng tổng hợp (tỉ lệ token theo loại, số lượng bài có tiếng Anh, số bài có phiên âm Hàn, …) được sử dụng để tạo các bảng thống kê và biểu đồ theo giai đoạn, thể loại, tác giả.

6. Các mô-đun chính và chức năng (Main Modules and Responsibilities)

Nhóm mô-đun thu thập dữ liệu gồm các script chuyên cho từng nguồn: crawler của Nhac.vn dựa trên sitemap để duyệt sâu toàn bộ cấu trúc trang và thu thập liên kết bài hát, sau đó module chi tiết bài hát tải metadata và lời; crawler của TKaraoke kết hợp ba chiến lược là mở rộng tiền tố tìm kiếm, quét ID tuần tự và tải metadata hàng loạt bằng xử lý song song, lưu trữ trong SQLite rồi chuyển ra CSV. Nhóm mô-đun chuẩn hóa và hợp nhất gồm các notebook xử lý ánh xạ cột, merge dữ liệu đa nguồn với phát hiện trùng lặp bằng tương đồng chuỗi, script điền năm phát hành dựa trên API ngoài và notebook khôi phục URL Nhac.vn bằng so khớp mờ giữa tiêu đề–lời hát. Nhóm mô-đun xây dựng từ điển chịu trách nhiệm tổng hợp từ điển Hán Việt từ nhiều nguồn, cùng với các notebook “trừ giao nhau” giữa các từ điển teencode, noise, tên riêng, tiếng Anh và phiên âm để đảm bảo tính phân tách giữa các lớp. Nhóm mô-đun gán nhãn và phân tích bao gồm notebook thực hiện pipeline 5 bước từ tiền xử lý đến gán nhãn đầy đủ cho từng token, các notebook sử dụng LLM (thông qua Groq hoặc mô hình Qwen chạy cục bộ) cho gợi ý phân loại bổ sung, và notebook phân tích–trực quan hóa kết quả cuối cùng.

7. Thuật toán / Mô hình sử dụng (Algorithms / Models Used)

Trong giai đoạn thu thập, các thuật toán sử dụng chủ yếu là duyệt sitemap theo chiều sâu và quét không gian truy vấn (mở rộng tiền tố tìm kiếm, quét dải ID URL) kết hợp với cơ chế checkpoint, xử lý song song và giới hạn tốc độ để ổn định khi chạy dài. Ở bước chuẩn hóa và hợp nhất, dự án dùng so khớp chuỗi dựa trên rapidfuzz để tính độ tương đồng giữa lời bài hát, kết hợp điều kiện trên tiêu đề và tác giả, nhằm nhận diện bản ghi trùng lặp; năm phát hành được suy luận bằng cách truy vấn nhiều API công khai, sau đó trích xuất năm bằng quy tắc biểu thức chính quy trên đoạn tóm tắt. Giai đoạn gán nhãn văn bản là phần có hàm lượng NLP cao nhất: hệ thống dùng chuẩn hóa Unicode, thay thế ký tự tương đương, rút gọn ký tự lặp, loại bỏ dấu câu để làm sạch, rồi áp dụng chuỗi bộ lọc từ điển theo thứ tự ưu tiên; các cụm phiên âm ngoại ngữ được tìm bằng cấu trúc Aho‑Corasick trên từ điển phức hợp, tên riêng được dò theo danh sách case‑sensitive, còn từ Hán Việt, từ Việt và từ Anh được nhận diện bằng tra cứu trong các từ điển tương ứng (bao gồm cả corpus words của NLTK). Bên cạnh pipeline dựa trên luật, dự án còn triển khai hai mô-đun dùng LLM: một mô-đun sử dụng các mô hình do Groq cung cấp (chuẩn hóa định dạng JSON đầu ra) và một mô-đun dùng Qwen2.5‑7B‑Instruct nén 4‑bit trên GPU, nhằm tự động trích xuất các thành phần như từ tiếng Anh, phiên âm và tên riêng từ lời bài hát; các kết quả này được dùng để hỗ trợ và kiểm tra chéo pipeline luật–từ điển hơn là thay thế hoàn toàn.

8. Dữ liệu vào và ra (Input and Output)

Dữ liệu vào của hệ thống chủ yếu là lời bài hát và metadata thu được từ các website âm nhạc Việt Nam, lưu dưới dạng CSV (mã hóa UTF‑8‑SIG để tương thích Excel) và cơ sở dữ liệu SQLite cho giai đoạn crawler. Bên cạnh đó, hệ thống còn tiêu thụ các file từ điển ở định dạng CSV hoặc TSV (từ điển Hán Việt, tiếng Anh, teencode, tên riêng, từ phiên âm, danh sách từ Việt) và phản hồi JSON từ các API bên ngoài như MusicBrainz, Wikipedia và iTunes phục vụ việc hoàn thiện thông tin năm phát hành. Dữ liệu đầu ra là một tập hợp các file CSV ở nhiều bước: dữ liệu thô theo từng nguồn, dữ liệu đã chuẩn hóa và hợp nhất, dữ liệu đã điền năm phát hành, dữ liệu đã khôi phục URL, từng phiên bản dữ liệu sau mỗi bước tiền xử lý–gán nhãn, và cuối cùng là bộ dữ liệu hoàn chỉnh chứa các cột thống kê số lượng/tỷ lệ token theo từng loại ngôn ngữ. Ngoài ra, notebook phân tích sinh ra các biểu đồ trực quan (dạng thanh, đường, stacked bar chuẩn hóa 100%) thể hiện xu hướng sử dụng tiếng Anh, Hán Việt, phiên âm Hàn/Nhật/Trung theo giai đoạn, theo thể loại và theo tác giả; các biểu đồ này có thể lưu ra tệp ảnh hoặc dùng trực tiếp trong báo cáo và poster.

9. Kịch bản sử dụng / Ứng dụng nghiên cứu (Use Case / Research Scenario)

Trong một kịch bản nghiên cứu điển hình, nhà nghiên cứu trước tiên khởi chạy các crawler để xây dựng một tập lời bài hát tiếng Việt đủ lớn, sau đó dùng tầng chuẩn hóa để hợp nhất, loại bỏ trùng lặp và bổ sung thông tin năm phát hành, tạo nên một “corpus ca khúc tiếng Việt” có chất lượng. Trên corpus này, pipeline gán nhãn được áp dụng để phân giải mỗi lời bài hát thành chuỗi token được phân loại theo nguồn gốc ngôn ngữ; từ đó, các chỉ số như tỷ lệ tiếng Anh, số bài có phiên âm tiếng Hàn, mật độ Hán Việt, … được tính toán cho từng bài, từng thể loại và từng giai đoạn thời gian. Các nhà nghiên cứu có thể sử dụng các bảng và hình vẽ đã sinh để lập luận về mức độ “xâm nhập” của văn hóa Mỹ–Anh, Hàn Quốc hay Trung Quốc vào lời ca, so sánh giữa các phong trào âm nhạc (ví dụ làn sóng K‑pop) hoặc giữa các nhóm nhạc sĩ, cũng như phục vụ làm bằng chứng định lượng cho các phân tích xã hội–văn hóa rộng hơn về âm nhạc đương đại Việt Nam.

10. Tái lập / Cách chạy dự án (Reproducibility / How to Run)

Để tái lập kết quả, người dùng cần chuẩn bị môi trường Python với các thư viện đã liệt kê (pandas, numpy, nltk, requests, BeautifulSoup, rapidfuzz, unidecode, matplotlib, seaborn, ahocorasick, groq, transformers, torch, …), đồng thời tải về corpora cần thiết của NLTK (đặc biệt là danh sách từ tiếng Anh). Với các chức năng gán nhãn bằng LLM, cần cấu hình khóa API Groq, hoặc truy cập môi trường GPU (ví dụ Google Colab) để tải và chạy mô hình Qwen2.5‑7B‑Instruct với thiết lập nén 4‑bit. Trình tự chạy được khuyến nghị theo ba giai đoạn: (i) chạy các script crawler nguồn Nhac.vn và TKaraoke để thu thập và xuất dữ liệu ra CSV, tận dụng chức năng checkpoint nếu cần dừng giữa chừng; (ii) mở và thực thi lần lượt các notebook chuẩn hóa và hợp nhất dữ liệu, bao gồm chuẩn hóa schema, merge đa nguồn, điền năm phát hành, khôi phục URL; (iii) chạy các notebook xây dựng từ điển và pipeline gán nhãn, sau đó là notebook phân tích–trực quan hóa để sinh các bảng thống kê và biểu đồ. Các tham số (như ngưỡng tương đồng, số lượng worker, giới hạn tốc độ) hiện được thiết lập trực tiếp trong mã nguồn và notebook; để tái lập chính xác kết quả trong báo cáo, cần giữ nguyên các ngưỡng và cấu hình như trong repository.
